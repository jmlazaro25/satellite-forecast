{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89924e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import data\n",
    "\n",
    "# Get daily grayscale rainfall data (https://neo.gsfc.nasa.gov/archive/gs/GPM_3IMERGDL/)\n",
    "iamges_dir = data.download()\n",
    "\n",
    "# Process downloaded data\n",
    "#iamges_dir = data.process_gs_rainfall_daily()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e9630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ConvRNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, kernal_size_, padding_):\n",
    "        super(ConvRNN, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.conv1 = nn.Conv2d(in_channels + hidden_channels, hidden_channels, kernel_size=kernal_size_, padding=padding_)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=kernal_size_, padding=padding_)\n",
    "        self.conv3 = nn.Conv2d(hidden_channels, in_channels, kernel_size=kernal_size_, padding=padding_)\n",
    "\n",
    "    def forward(self, in_tensor, h):\n",
    "        h = torch.cat((in_tensor, h), dim=1)\n",
    "        h = torch.relu(self.conv1(h))\n",
    "        h = torch.relu(self.conv2(h))\n",
    "        return self.conv3(h), h\n",
    "\n",
    "    def init_hidden(self, batch_size, img_size):\n",
    "        return torch.zeros(batch_size, self.hidden_channels, *img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b1fbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "def rolling_batch(files_list, start, stop, seq_len):\n",
    "    \n",
    "    # Load images\n",
    "    batch_images = np.array([plt.imread(files_list[i]) for i in range(start, stop)])\n",
    "    image_size = batch_images.shape[-2:]\n",
    "    \n",
    "    # TEMPORARILY crop images here - will be done in a preprocessing step once effects on model are clear\\n\",\n",
    "    image_size = batch_images.shape[-2:]\n",
    "    batch_images = batch_images[:, :image_size[0]//2, :image_size[1]//2]\n",
    "    image_size = batch_images.shape[-2:]\n",
    "\n",
    "    # Explicit single gs channel\n",
    "    batch_images = batch_images.reshape(len(batch_images), 1, *image_size)\n",
    "\n",
    "    # Organize into (batch_size, seq_len, channels=1, *image_size) (X and y)\n",
    "    batch_size = stop - start - seq_len\n",
    "    X = np.array([\n",
    "                    batch_images[seq_n : seq_n + seq_len]\n",
    "                    for seq_n in range(batch_size)\n",
    "                    ])\n",
    "    y = np.array([\n",
    "                    batch_images[seq_n + seq_len]\n",
    "                    for seq_n in range(batch_size)\n",
    "                    ])\n",
    "\n",
    "    return torch.from_numpy(X), torch.from_numpy(y)\n",
    "\n",
    "\n",
    "# Test rolling_batch\n",
    "n_images_test = 50\n",
    "test_image_files = sorted(glob(iamges_dir + '/*.PNG'))[-n_images_test:]\n",
    "X, y = rolling_batch(test_image_files, 1, 5, 2)\n",
    "assert X.shape == (2, 2, 1, 900, 1800), X.shape\n",
    "assert y.shape == (2, 1, 900, 1800), y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e58310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, seq_len):\n",
    "    \"\"\" For a single batch \"\"\"\n",
    "\n",
    "    hidden = model.init_hidden(X.shape[0], X.shape[-2:])\n",
    "\n",
    "    for step in range(seq_len - 1):\n",
    "        _, hidden = model(X[:,step], hidden)\n",
    "\n",
    "    pred, _ = model(X[:, -1], hidden)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20a3a83e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'X_val' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m crnn \u001b[38;5;241m=\u001b[39m ConvRNN(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, kernal_size_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, padding_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#%time \u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_image_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, files_list, train_frac, val_frac, seq_len, batch_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_frac \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     45\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 46\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m predict(model, \u001b[43mX_val\u001b[49m, seq_len)\n\u001b[1;32m     47\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_val)\n\u001b[1;32m     48\u001b[0m     val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'X_val' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "def train(model, files_list, train_frac, val_frac, seq_len, batch_size):\n",
    "\n",
    "    n_images = len(files_list)\n",
    "    train_n = int(train_frac * n_images)\n",
    "    images_per_batch = seq_len + batch_size\n",
    "    n_batches = train_n / images_per_batch\n",
    "\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    train_losses = []\n",
    "    \n",
    "    if val_frac != 0:\n",
    "        X_val, y_val = rolling_batch(files_list, train_n, int((train_frac + val_frac)*n_images), seq_len)\n",
    "        val_losses = []\n",
    "\n",
    "    for batch_n in range(int(n_batches)):\n",
    "        \n",
    "        # Use smaller final batch if needed\n",
    "        # Use range(ceil(n_batches)) when correcting this\n",
    "        # if batch_n == ceil(n_batches):\n",
    "        #    batch_size = train_n - int(n_batches) * images_per_batch - seq_len\n",
    "        #    print(batch_size) # needs to be passes (and used) in rolling_batch\n",
    "        \n",
    "        # Make training batch\n",
    "        start_batch_ind = batch_n * images_per_batch\n",
    "        stop_batch_ind = (batch_n + 1) * images_per_batch\n",
    "        X_train, y_train = rolling_batch(files_list, start_batch_ind, stop_batch_ind, seq_len)\n",
    "\n",
    "        # Predictions and loss on training batch\n",
    "        y_pred = predict(model, X_train, seq_len)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        train_losses.append(train_loss.item())\n",
    "        del X_train, y_train, y_pred\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation if not refitting\n",
    "        if val_frac != 0:\n",
    "            model.eval()\n",
    "            y_pred = predict(model, X_val, seq_len)\n",
    "            val_loss = criterion(y_pred, y_val)\n",
    "            val_losses.append(val_loss.item())\n",
    "            del X_val, y_val, y_pred, val_loss\n",
    "            model.train()\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Test train function\n",
    "crnn = ConvRNN(in_channels=1, hidden_channels=2, kernal_size_=3, padding_=1)\n",
    "#%time \n",
    "train_loss, val_loss = train(crnn, test_image_files, 0.6, 0.2, 5, 5)\n",
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(val_loss, label='Validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af67b9",
   "metadata": {},
   "source": [
    "### Real training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae33512",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 500\n",
    "image_files = sorted(glob(iamges_dir + '/*.PNG'))[-n_images:]\n",
    "\n",
    "crnn = ConvRNN(in_channels=1, hidden_channels=2, kernal_size_=3, padding_=1)\n",
    "%time train_loss, val_loss = train(crnn, image_files, 0.6, 0.2, 5, 5)\n",
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(val_loss, label='Validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3be17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
